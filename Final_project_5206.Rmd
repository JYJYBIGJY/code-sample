---
title: "Final_Exam_5206_4206_Fall_2021"
author: "Gabriel Young"
date: "12/17/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part I: Inverse Transform Method

Consider the continuous random variable $X$ with density function: 
\[
f(x) = \begin{cases} \frac{2}{x^3}, \ \ \  x\geq 1 \\
\  0, \ \ \ \ \text{otherwise}
\end{cases}
\]
```{r}
f <- function(x) {
  out <- ifelse(x<1,0,2/x^3)
  return(out)
}
x.plot <- seq(-1,15,length=1000)
plot(x.plot,f(x.plot),type="l",ylim=c(-.2,2.2),xlab="x",ylab="f(x)")
abline(h=0,lty=2)
```

## Problem 1

Analytically derive the cdf of $X$, i.e., find $F(x)$. Plot the cdf $F(x)$ over the interval $[-1,15]$. You only have to show the plot of $F(x)$ for submission.  

**Note:** The below derivation is not required for submission but is included for reference. 
\[
F(x)=\int_{1}^{x} \frac{2}{t^3} dt = \begin{cases} 1- \frac{1}{x^2}, \ \ \  x\geq 1 \\
\  0, \ \ \ \ \ \ \text{otherwise}
\end{cases}
\]

**Solution**
```{r}
### Solution goes here -------------------------
f <- function(x){
  out <- ifelse(x >= 1,1-1/x^2,0)
  return(out)
}
x <- seq(-1,15, by = 0.01)
plot(x,f(x),type = "l",xlab="x",ylab="F(x)")
```

## Problem 2

Simulate 10,000 cases from $f(x)$ using the inverse transform method. Plot a histogram of the simulated cases with the true cdf overlayed on the plot.  Also, on a separate graphic, plot the empirical cdf of your simulated cases. **Note:** make sure to truncate your x-axis by setting **xlim=c(0,15)**.

**Solution**
```{r}
### Solution goes here -------------------------
f.sim <- function(n){
  U <- runif(n)
  return(ifelse((U<0|U>1),0,sqrt(1/(1-U))))
}
f2 <- function(x) {
  out <- ifelse(x<1,0,2/x^3)
  return(out)
}
draws <- f.sim(10000)
x <- seq(0,15,by = 0.01)
hist(draws,xlim = c(0,15),ylim = c(-.2,2.2),breaks = 600,prob = T,xlab = "x")
lines(x,f2(x),col = "red")
plot(ecdf(draws),main = "empirical cdf of simulated cases",xlim = c(0,15))
```


# Part II: Monte Carlo Integration and Accept-Reject

## Problem 3

Consider the probability density function 
\[
f(x) = \begin{cases} \frac{3}{2}cos(x)(sin(x))^2, \ \ \  \ 3\pi/2 \leq x \leq 5\pi/2 \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 0, \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{otherwise.}
\end{cases}
\]
Define the function $f(x)$ and evaluate the points $f(2\pi),f(7)$. Note that $f(x)$ is not the same density as problems 1-2. 

**Solution**
```{r}
### Solution goes here -------------------------
f <- function(x){
  out <- ifelse(x < 3*pi/2| x> 5*pi/2,0,3/2*cos(x)*(sin(x))^2)
  return(out)
}

f(2*pi)
f(7)
```

## Problem 4

Plot $f(x)$ over the interval $[3,10]$.

**Solution**
```{r}
### Solution goes here -------------------------
x <- seq(3,10,by = .001)
plot(x,f(x),type = "l")
```

## Problem 5

Use Monte Carlo Integration to show that $f(x)$ is a valid probability density function, i.e., show that 
\[
\int_{3\pi/2}^{5\pi/2} f(x) dx = 1.
\]
Note the Monte Carlo method will approximate the above integral and your result should be very close to 1. Pick $n$ large enough so that the Monte Carlo integral is within .001 of the truth. 

**Solution**
```{r}
### Solution goes here -------------------------
n <- 1000^2
x <- runif(n,3*pi/2,5*pi/2)
mean(f(x)*pi)

```

## Problem 6

Use the accept-reject method to simulate 10,000 draws from $f(x)$. Your envelope function must satisfy $e(x) \geq f(x)$ for all $x$ but it does not have to be perfect.  


**Solution**
```{r}
### Solution goes here -------------------------
e <- function(x) {
  ifelse(x < 3*pi/2| x> 5*pi/2,Inf,0.59)
}
x <- seq(3,10,by = .01)
plot(x,f(x),type = "l",main = "test e over f")
lines(x,e(x),col = "red")

n.samps <- 10000 # Samples desired
n <- 0 # counter for number samples accepted
f.draws <- numeric(n.samps) # initialize the vector of output
while (n < n.samps) {
  y <- runif(1, min = 3*pi/2, max = 5*pi/2) # random draw from g
  u <- runif(1)
  if (u < f(y)/e(y)) {
    n <- n + 1
    f.draws[n] <- y
    }
}
# check simulation
hist(f.draws,probability = T,breaks = 50,xlim = c(3,10))
```

# Part III: Simulating an AR(1) Proccess

Consider the autoregressive lag 1 model (AR(1)) defined by 
\[
\epsilon_t = \phi \epsilon_{t-1}+z_t, \ \ \ t=2,3,\ldots,n,
\]
and 
\[
z_t \overset{iid}{\sim}N(0,\sigma^2), \ \ \ \epsilon_1=\text{constant}.
\]
The autoregressive parameter $\phi$ must satisfy $|\phi|<1$, otherwise the series will exhibit unpredictable behavior, i.e., random walk ($|\phi|=1$) or explosive ($|\phi|>1$).  

## Problem 7

Create a function named **my_AR1** that simulates $n$ realizations from the AR(1) model. Your function should have inputs **n, phi, sigma, e1**; where **n** is the number of simulated cases, **phi** is the AR(1) parameter, **sigma** is the noise standard deviation and **e1** is the initial value with default **e1=0**. 
Test your function using **set.seed(2)** and inputs **my_AR1(n=150,phi=.9,sigma=2,e1=0)**. Plot your simulated AR(1) process over time, i.e., plot your AR(1) process over the sequence **1:150**. 

**Solution**
```{r}
### Solution goes here -------------------------
my_AR1 <- function(n,phi,sigma,e1=0){
  e <- numeric(n)
  e[1] <- e1
  for (i in 2:n){
    z <- rnorm(1, 0, sigma)
    e[i] = phi*e[i-1] + z  
  }
  return(e)
}
set.seed(2)
AR1_sim <- my_AR1(n=150,phi=.9,sigma=2,e1=0)
plot(seq(150),AR1_sim,xlab = "t",ylab = "et",type= "l")
```

# Part IV: Maximum Likelihood Estimation

Consider estimating the noise variance $\sigma^2$ and the AR(1) parameter $\phi$ using maximum likelihood estimation. This problem is non-trivial and requires some heavy prerequisite knowledge. The following definitions lack complete descriptions and students must take these formulas on faith. You can learn more about these relations in a time series course. 

The AR(1) likelihood function of interest can be formulated by:
\[
\mathcal{L}(\sigma^2,\phi; \epsilon_1,\cdots,\epsilon_n) = {\huge{[}} \prod_{t=2}^n f(\epsilon_t|\epsilon_{t-1};\sigma^2,\phi) {\huge{]}}*f(\epsilon_1;\sigma^2,\phi)
\]
Further, the conditional distribution of $\epsilon_t|\epsilon_{t-1}$ is 
\[
\epsilon_t|\epsilon_{t-1} \sim N(0,\sigma^2),
\]
and the distribution of $\epsilon_1$ is 
\[
\epsilon_1 \sim N(0,\sigma^2/(1-\phi^2)).
\]
The likelihood can be formulated by a product of normal densities (**Hint: dnorm()**). 

## Problem 8 

Define the function **L.AR1** which computes the likelihood $\mathcal{L}(\sigma^2,\phi; \epsilon_1,\cdots,\epsilon_n)$ of the AR(1) model. **L.AR1** should be a function of the parameter vector **theta** and data vector **ts_data**. Note that **theta** must be a vector of length 2, i.e., $\theta= (\sigma^2, \phi)$. 

Test your function **L.AR1** on the simulated time series **AR1_sim** from problem 7 using inputs **L.ar1(theta=c(5,.9),ts_data=AR1_sim)**. Note that your function should return a very small number on the order of **10^{-145}**. 

**Solution**
```{r}
### Solution goes here -------------------------
L.AR1 <- function(theta,ts_data){
  s <- theta[1]
  p <- theta[2]
  out <- dnorm(ts_data[1],0,sqrt(s/(1-p^2)))
  for (i in 2:length(ts_data)){
    out <- out*dnorm(ts_data[i],mean = p*ts_data[i-1],sqrt(s))
  }
  return(out)
}
L.AR1(theta=c(5,.9),ts_data=AR1_sim)
```

## Problem 9

Define the function **neg.ll.AR1** which computes the negative log-likelihood of the AR(1) model. Similar to problem 8, **neg.ll.AR1** should be a function of the parameter vector **theta** and data vector **ts_data**. Test your function **neg.ll.AR1** on the simulated time series **AR1_sim** from problem 7 using inputs **neg.ll.ar1(theta=c(5,.9),ts_data=AR1_sim)**. Note that your function should return a number around 333. 

**Solution**
```{r}
### Solution goes here -------------------------
neg.ll.AR1 <- function(theta,ts_data){
  s <- theta[1]
  p <- theta[2]
  out <- dnorm(ts_data[1],0,sqrt(s/(1-p^2)),log = T)
  for (i in 2:length(ts_data)){
    out <- out+dnorm(ts_data[i],mean = p*ts_data[i-1],sqrt(s),log = T)
  }
  return(-out)
}
neg.ll.AR1(theta=c(5,.9),ts_data=AR1_sim)
```

## Problem 10

Use the **nlm()** function to minimize the negative log-likelihood **neg.ll.ar1** based on the simulated AR(1) model **AR1_sim**. Minimizing **neg.ll.ar1** will yield the maximum likelihood estimates of noise variance $\sigma^2$ and autoregressive parameter $\phi$. Before minimizing the negative log-likelihood, make sure to center the **AR1_sim** dataset and use starting value **p=c(5,.9)**. Note that there will be at least one warning from the **nlm()** output, which is suppressed using **warning=F**. 

Compare your estimated values MLE values $\hat{\sigma}_{mle}$ and $\hat{\phi}_{mle}$ to the true parameters $\sigma=2$ and $\phi=.9$. Also compare the sample variance of **AR1_sim** with the quantity 
\[
\frac{\hat{\sigma}_{mle}^2}{1-\hat{\phi}_{mle}^2}.
\]

**Solution**
```{r,warning=F}
### Solution goes here -------------------------
nlm(neg.ll.AR1,c(5,.9),ts_data = AR1_sim)$estimate
est <- nlm(neg.ll.AR1,c(5,.9),ts_data = AR1_sim)$estimate
# the estimated std is sqrt(4.91) which is about 2.22 and the estimated phi is 0.847.
# compare to the true parameters, the std is a bit larger and the phi is a bit smaller. 
var(AR1_sim) ; est[1]/(1-est[2]^2)
# they are vary close
```

# Part V: SPLIT/APPLY/COMBINE

Suppose that you are data scientist working at Boeing Airlines (BA). Further, suppose that you are interested in how your company's stock (BA) correlates with other major cooperate entities included in the Dow Jones Industrial Average (DJIA). The csv file **close_data.csv** includes the daily closing prices of the Dow Jones Industrial Average (DJIA) recorded from 2021-01-01 to 2021-11-02. Each stock is recorded over 210 trading days. Among the 30 companies in the DJIA, Boeing (BA) is removed, resulting in 29 tickers.  The closing prices of Boeing (BA) are summarized in a separate csv file **BA.csv**. 

```{r}
BA <- read.csv("BA.csv")
dim(BA)
close_data <- read.csv("close_data.csv")
dim(close_data)
names(close_data)
```

## Problem 11

Create a new dataframe named **df_AXP** that contains only the rows related to ticker **AXP**. Your sub-dataframe should have dimension $(210\times3)$. Show the **head** and **dim** of the dataframe **df_AXP**. 

**Solution**
```{r}
### Solution goes here -------------------------
df_AXP <- close_data[close_data$ticker == "AXP",]
```

## Problem 12

For a given day $(t)$, the returns $(R_t)$ of a financial object are defined by
\[
R_t=\frac{P_{t+1}-P_{t}}{P{_t}}, \ \ t=1,\ldots,n-1.
\]
In our setting, the closing price represents $P_t$ and the data is recorded over $n=210$ trading days, i.e., $t=1,2,\ldots,209$. Define a function **compute_return** that computes a vector of the returns of a specific stock and appends this column onto your existing dataframe. Test your function **compute_return** on the **df_AXP** dataframe from problem 11, which should yield in a new dataframe of dimension $(209\times4)$. Show the **head** and **dim** of **compute_return(df_AXP)**. 

**Solution**
```{r}
### Solution goes here -------------------------

compute_return <- function(df){
  result <- vector(length = 210)
  for (i in 1:209){
    result[i+1] <- (df$close[i+1]-df$close[i])/df$close[i]
  }
  df$return <- result
  df <- df[-1,]
  return(df)
}
df_AXP <- compute_return(df_AXP)
head(df_AXP)
dim(df_AXP)

```

## Problem 13

Create a new **BA** dataframe by evaluating **compute_return(BA)**. Assign the updated BA dataframe as **BA_new**. Again, show the **head** and **dim** of **compute_return(BA)**. 

**Solution**
```{r}
### Solution goes here -------------------------
BA_new <- compute_return(BA)
head(BA_new)
dim(BA_new)
```

## Problem 14

Run the **SPLIT/APPLY/COMBINE** model on the full dataset **close_data**, i.e., split by **ticker** and apply the function **compute_return** on each sub-dataframe. To solve this problem, use an appropriate function from the **plyr** package in conjunction with your **compute_return** function. Assign your new dataframe as **close_data_new** and compute its dimension. Note that $209*29=6061$.

**Solution**
```{r}
library("plyr")
### Solution goes here -------------------------
df.split <- split(close_data,close_data$ticker)
close_data_new <- ldply(df.split,compute_return)
close_data_new <- close_data_new[,-1]
dim(close_data_new)
```

# Part VI: More SPLIT/APPLY/COMBINE

Suppose that Boeing has a fixed amount of money to invest into itself ($Y$) and another company ($X$). We must decide what fraction ($\alpha$) of money to invest in $Y=BA$ and $(1-\alpha)$ in stock $X$. The total returns ($W$) are modeled by the relation 
\[
W=\alpha X+(1-\alpha) Y.
\]
To determine the optimal percentage $\alpha$, we minimize the variance of $W$, i.e., minimize the expression 
\[
var(W)=\alpha^2\sigma^2_X+(1-\alpha)^2\sigma^2_Y+2 \alpha (1-\alpha) cov(X,Y).
\]
The univariate derivative of total variance $var(W)$ with respect to $\alpha$ is:  
\[
\frac{d}{d \alpha} var(W) = 2 \alpha \sigma^2_X+2 \alpha \sigma^2_Y - 2 \sigma^2_Y + 2 cov(X,Y) -4 \alpha cov(X,Y)
\]
Consequently, the optimal value of $\alpha$ is  
\[
\alpha = \frac{\sigma^2_Y-cov(X,Y)}{\sigma^2_X+\sigma^2_Y-2 cov(X,Y)}.
\]
Hence, we can choose a reasonable estimator of $\alpha$ as
\[
\hat{\alpha} = \frac{\hat{\sigma}^2_Y-\hat{cov}(X,Y)}{\hat{\sigma}^2_X+\hat{\sigma}^2_Y-2 \hat{cov}(X,Y)}.
\]
In **R**, you can easily estimate $\alpha$ using:
```{r}
#(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))

```

## Problem 15

Compute the estimated $\alpha$ based on **Y=BA** versus **X=AXP**.  Conclude what percentage to invest in each stock. 

**Solution**
```{r}
### Solution goes here -------------------------
Y = BA_new$return
X = df_AXP$return
(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
1-(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
# 71.338% in AXP and 28.662% in BA
```

## Problem 16

Now suppose that Boeing is interested in a stock buy-back but also wants to invest in one other company. To determine which company to invest in, the data scientist compares the sample correlations of $BA$ versus all other tickers in the DJIA. Further, you also store the estimated $\alpha$ value for each DJIA ticker verses **BA**. Define a function **BA_relationships** with inputs: **ticker**, **BA** and **data**.  The **BA_relationships** function should output a vector of length 2 with the sample correlation and estimated alpha value. Test **BA_relationships** on the ticker **AXP**. 
 

**Solution**
```{r}
### Solution goes here -------------------------
BA_relationships <- function(ticker,BA,data){
  result <- vector(length = 2)
  company <- data[data$ticker == ticker,]
  Y = BA_new$return
  X = company$return
  result[1] <- cor(X,Y)
  result[2] <- (var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
  return(result)
}

BA_relationships("AXP",BA,data = close_data_new)

#out <- data.frame(row.names = tickers)
#for (i in tickers){
#  company <- close_data_new[close_data_new$ticker == i,]
#  out[i,1] <- cor(company$return,Y)
#}
#out
#max(out[,1])
```

# Problem 17

Run a SPLIT/APPLY/COMBINE procedure to store sample correlations of $BA$ verses all stocks and the corresponding estimated $\alpha$ values. Display the head and dimension of your resulting dataframe after sorting its rows by correlation. Note that your resulting dataframe should have dimension $(29\times2)$ and **GS** has the highest correlation with **BA** among all tickers in the DJIA. 


**Solution**
```{r}
### Solution goes here -------------------------
companies <- split(close_data_new,close_data_new$ticker)
coralpha <- function(df){
  result <- vector(length = 2)
  Y = BA_new$return
  X = df$return
  result[1] <- cor(X,Y)
  result[2] <- (var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
  return(result)
}
relation <- ldply(companies,coralpha)
colnames(relation) <- c("ticker","cor","alpha")
relation <- relation[order(relation$cor,decreasing = T),]
head(relation)
dim(relation)
```

# Part VII: MCMC of the AR(1) Model

Consider the linear regression model:
\[
Y_t=\beta_0+\beta_1 X_t + \epsilon_t, \ \ \ t=1,2,\ldots,n,
\]
where $Y_t$ is the closing price of Boeing (**BA**) and $X_t$ is the closing price of Goldman Sachs (**GS**), each measured on day $t$.  Below shows the linear regression analysis on $Y$ versus $X$ using the **lm()** function. The vector **res** represents the residuals of your linear model.  

**Note:** we are using the **closing price** and not the **returns** for this analysis.  

```{r}
# Define Y
Y <- BA$close

# Define X
GS_data <- close_data[close_data$ticker=="GS",]
X <- GS_data$close

# Run linear model
model <- lm(Y~X)

# Plot Y versus X
plot(X,Y,xlab="GS closing price",ylab="BA closing price")
abline(model,col="red")

# define residuals
res <- residuals(model)

# Display coefficients 
model$coefficients

# Plot residuals 
plot(res,type="l",main="Residuals of lm(BA~GS)")
```

Further, suppose that the error structure $\epsilon_t$ is modeled by an autoregressive lag 1 (AR(1)) process defined by 
\[
\epsilon_t = \phi \epsilon_{t-1}+z_t, \ \ \ t=2,3,\ldots,n \\
z_t \overset{iid}{\sim}N(0,\sigma^2)
\]
The full statistical model is comprised 4 parameters: $\beta_0,\beta_1,\sigma^2,\phi$. For Problems 18 and 19, we restrict our attention to just the noise variance $\sigma^2$ and the AR(1) parameter $\phi$, i.e., $\theta=(\sigma^2,\phi)$. 

# Problem 18

Estimate the noise variance $\sigma^2$ and the AR(1) parameter $\phi$ of the residuals **res** via maximum likelihood. You can easily solve this problem by minimizing **neg.ll.ar1** from problems 9 and 10. Display your estimated parameters based the **nlm()** output. 

**Solution**
```{r, warning=F}
### Solution goes here -------------------------
nlm(neg.ll.AR1,c(5,.9),ts_data = res)$estimate

```

# Problem 19

Consider using a Bayesian approach to model the AR(1) error structure introduced in Part VII. Assuming the priors 
\[
\sigma^2 \sim \text{gamma}(\alpha=5,\beta=4),
\]
and
\[
\phi \sim \text{unif}(0,1),
\]
your goal is to estimate the posterior $\pi(\theta | e_1,e_2,\ldots,e_n)$, where $\theta$ is the parameter vector $\theta=(\sigma^2,\phi)$ and $e_1,e_2,\ldots,e_n$ are the residuals **res**. Using Markov Chain Monte Carlo, estimate the posterior using the Metropolis Hastings algorithm from page 65 of SET11 lecture notes. Your simulated posterior $\theta_{(t)}$ should be estimated using 100,000 MCMC iterations and discarding a 20% burn-in period. The resulting matrix $\theta_{(t)}$ will have dimension $(2\times 80000)$. 

**Note:** If your chain fails to run, try running it a few times to see if it recovers from the initial draw $\theta^{(0)}$.  You should be able to set some seed so that your Markdown file always knits. 

Display traceplots and histograms of chains $\sigma^2_{(t)}$ and $\phi_{(t)}$. 

**Solution**
```{r}
### Solution goes here -------------------------
# Draw case for t=0 from proposal
set.seed(1)
theta_1 <- c(rgamma(1,shape=5,scale=4),runif(1)) # Draw theta_(0)  
n.samps <- 100000 # Number of iterations
theta_vec <- vector("list",length = n.samps+1)
theta_vec[[1]] <- theta_1

res <- unname(res)
# MCMC loop
for (t in 1:n.samps) {
  theta_star <- c(rgamma(1,shape=5,scale=4),runif(1)) # Draw theta* from from proposal
  theta_t <- theta_vec[[t]] # theta_(t) 
  # Compute MH ratio
  MH_ratio <- L.AR1(theta_star,res)/L.AR1(theta_t,res)
  # Select new case 
  prob_vec <- c(min(MH_ratio,1),1-min(MH_ratio,1))
  index <- sample(c(1,2),1,prob = prob_vec) 
  if(index == 1){theta_vec[[t+1]] <- theta_star}
  else{theta_vec[[t+1]] <- theta_t}
}

# change list to dataframe
thetadf <- data.frame(matrix(unlist(theta_vec), nrow=length(theta_vec), byrow=TRUE))
# sigma^2
plot(thetadf[,1],type="l",main = "sigma^2",xlab = "sigma^2")
hist(thetadf[,1],breaks=30,main = "sigma^2",xlab ="sigma^2")

# phi
plot(thetadf[,2],type="l",main = "phi",xlab = "phi")
hist(thetadf[,2],breaks=30,main = "phi",xlab = "phi")



```

# Problem 20

Compute the Bayes' estimates for both $\sigma^2$ and $\phi$ based on your simulated chain, after discarding the 20% burn-in period. **Note:** Simply take the sample mean of your simulated chains! 

**Solution**
```{r}
### Solution goes here -------------------------
mean(thetadf[c(20000:100001),1])
mean(thetadf[c(20000:100001),2])
```

# Part VIII: Forecasting 

Consider the csv file **Pred_BA_GS.csv**, which includes the closing prices from **Y=BA** and **X=GS** recorded over the time period 2021-11-03 to 2021-12-30. The dataframe has dimension  $(31 \times 2)$. 

```{r}
pred_data <- read.csv("Pred_BA_GS.csv")
#dim(pred_data)
Y_test <- pred_data$BA
X_test <- pred_data$GS
plot(X_test,Y_test,xlab="GS closing price",ylab="BA closing price",main="Prediciton Set")
```

# Problem 21

For this task, you will forecast the closing price of Boeing (**BA**) based on the linear regression model 
\[
Y_t=\beta_0+\beta_1 X_t + \epsilon_t,
\]
with AR(1) errors
\[
\epsilon_t = \phi \epsilon_{t-1}+z_t, 
\]
\[
z_t \overset{iid}{\sim}N(0,\sigma^2)
\]
To accomplish this task:
1) Manually compute the linear component's prediction of $Y=BA$ using $Y_{pred}=\hat{\beta}_0+\hat{\beta}_1 X$, where $X$ represents the test cases of $GS$. This should result in a vector of length 31. 
2) Compute the vector of estimated errors or residuals **res_pred <- Y_test - Y_pred**. 
3) Forecast $Y$ by combining $Y_{pred}$ from step (1) and the estimated errors from step (2). 
4) Compute the mean square prediction error of your forecasted $BA$ values versus the true values **Y_test**.  Compare this number to $\hat{\sigma}^2$. 
5) Create a graphic showing the difference between your forecasted $BA$ values and true values **Y_test**.

**Notes:** Do not over-complicate this problem! You will directly use $\hat{\beta}_0$ and $\hat{\beta}_1$  from the linear model introduced in **Part VII**. Further you can choose the estimated AR(1) coefficient as the MLE from Problem 18 or the Bayes' estimate from problem 20. If you failed to complete 18 or 19, use $\hat{\phi}=.9$.  

**Solution**
```{r}
### Solution goes here -------------------------
#1
summary(lm(Y_test~X_test))$coefficients
b0.hat <- -103.21
b1.hat <- 0.79
Y_pred <- b0.hat+b1.hat*X_test
Y_pred
#2
res_pred <- Y_test-Y_pred
#3
# i use the result from problem 18
e <- numeric(32)
e[1] <- 0 
set.seed(1)
for (i in 2:32){
  e[i] <- e[i-1]*0.958+rnorm(1,mean = 0,sd = 21.445)
}
e <- e[-1]
Y_pred_3 <-Y_pred + e
#4
sum((Y_test-Y_pred_3)^2)/31
#5
diff <- Y_test-Y_pred_3
plot(diff~seq(31),xlab = "day")
```

